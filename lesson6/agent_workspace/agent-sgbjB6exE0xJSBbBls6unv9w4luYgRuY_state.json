{
    "tools": null,
    "model_name": null,
    "search_algorithm": null,
    "artifacts_file_extension": null,
    "mcp_url": null,
    "tool_call_summary": true,
    "docs_folder": null,
    "plan_enabled": false,
    "custom_loop_condition": null,
    "tool_schema": null,
    "memory_chunk_size": 2000,
    "template": null,
    "preset_stopping_token": false,
    "handoffs": null,
    "verbose": true,
    "pdf_path": null,
    "task": null,
    "retry_interval": 1,
    "dashboard": false,
    "output_cleaner": null,
    "best_of_n": null,
    "safety_prompt_on": false,
    "random_models_on": false,
    "show_tool_execution_output": true,
    "stopping_token": "<DONE>",
    "react_on": false,
    "streaming_on": false,
    "function_calling_format_type": "OpenAI",
    "load_state_path": null,
    "reasoning_enabled": false,
    "workspace_dir": "agent_workspace",
    "long_term_memory": null,
    "rag_every_loop": false,
    "current_model_index": 0,
    "context_length": 200000,
    "tool_choice": "auto",
    "evaluator": null,
    "fallback_models": [],
    "all_gpus": false,
    "model_attempts": {},
    "print_on": true,
    "limit_tokens_from_string": null,
    "artifacts_on": false,
    "all_cores": true,
    "parser": null,
    "callbacks": null,
    "id": "agent-f207af7f3b514433b9264bfd8d8b17cb",
    "do_not_use_cluster_ops": true,
    "temperature": 0.5,
    "autosave": true,
    "return_step_meta": false,
    "description": null,
    "presence_penalty": 0.6,
    "llm_args": null,
    "list_base_models": null,
    "output_raw_json_from_tool_call": false,
    "drop_params": true,
    "artifacts_output_path": null,
    "device": "cpu",
    "tools_list_dictionary": null,
    "agent_ops_on": false,
    "list_of_pdf": null,
    "frequency_penalty": 0.8,
    "sentiment_threshold": null,
    "return_history": false,
    "conversation_schema": null,
    "loop_interval": 0,
    "custom_exit_command": "exit",
    "sop_list": null,
    "mcp_config": null,
    "traceback_handlers": null,
    "thinking_tokens": null,
    "custom_planning_prompt": null,
    "mcp_urls": null,
    "llm_api_key": null,
    "tool_retry_attempts": 3,
    "agent_description": null,
    "data_memory": null,
    "mcp_configs": null,
    "dynamic_loops": false,
    "system_prompt": "\nYou are an expert problem-solving agent designed to not only solve complex problems but also critically evaluate the quality of your thought process and final answers. \nYour task is to follow a structured approach to generate solutions, assess your thoughts, and provide a rating for each on a scale of 0.1 to 1.0. \nThis rating should reflect the accuracy and quality of your reasoning and final answer.\n\n### Instructions:\n\n1. **Understand the Problem:**\n   - Carefully analyze the problem provided by the user.\n   - Break down the problem into smaller, manageable parts if necessary.\n   - Formulate a clear understanding of the problem before proceeding.\n\n2. **Generate Thoughts:**\n   - Create multiple thoughts or steps toward solving the problem.\n   - For each thought, document your reasoning, ensuring that it is logical and well-founded.\n\n3. **Self-Evaluation:**\n   - After generating each thought, evaluate its accuracy and quality.\n   - Assign an evaluation score between 0.1 and 1.0. Use the following guidelines:\n     - **0.1 to 0.4:** The thought is flawed, inaccurate, or incomplete.\n     - **0.5 to 0.7:** The thought is partially correct but may lack detail or full accuracy.\n     - **0.8 to 1.0:** The thought is accurate, complete, and well-reasoned.\n\n4. **Generate Final Answer:**\n   - Based on your thoughts, synthesize a final answer to the problem.\n   - Ensure the final answer is comprehensive and addresses all aspects of the problem.\n\n5. **Final Evaluation:**\n   - Evaluate the overall quality and accuracy of your final answer.\n   - Provide a final evaluation score based on the same 0.1 to 1.0 scale.\n   \n",
    "output_type": "str-all-except-first",
    "rules": null,
    "custom_tools_prompt": null,
    "max_tokens": 4096,
    "step_pool": [],
    "user_name": "swarms_corp",
    "sop": null,
    "docs": null,
    "fallback_model_name": null,
    "traceback": null,
    "sentiment_analyzer": null,
    "top_p": 0.9,
    "multi_modal": null,
    "planning_prompt": null,
    "chain_of_thoughts": false,
    "max_loops": 1,
    "function_calling_type": "json",
    "timeout": null,
    "capabilities": null,
    "scheduled_run_date": null,
    "response_filters": [],
    "state_save_file_type": "json",
    "feedback": [],
    "reasoning_prompt_on": true,
    "saved_state_path": "agent-sgbjB6exE0xJSBbBls6unv9w4luYgRuY_state.json",
    "tree_of_thoughts": false,
    "log_directory": null,
    "stopping_condition": null,
    "load_yaml_path": null,
    "interactive": false,
    "transforms": null,
    "created_at": 1761644449.895473,
    "use_cases": null,
    "role": "worker",
    "callback": null,
    "tags": null,
    "llm_base_url": null,
    "tool_system_prompt": "\n\n\n    You've been granted tools to assist users by always providing outputs in JSON format for tool usage. \n    Whenever a tool usage is required, you must output the JSON wrapped inside markdown for clarity. \n    Provide a commentary on the tool usage and the user's request and ensure that the JSON output adheres to the tool's schema.\n    \n    Here are some rules:\n    Do not ever use tools that do not have JSON schemas attached to them.\n    Do not use tools that you have not been granted access to.\n    Do not use tools that are not relevant to the task at hand.\n    Do not use tools that are not relevant to the user's request.\n    \n    \n    Here are the guidelines you must follow:\n\n    1. **Output Format**:\n    - All outputs related to tool usage should be formatted as JSON.\n    - The JSON should be encapsulated within triple backticks and tagged as a code block with 'json'.\n\n    2. **Schema Compliance**:\n    - Ensure that the JSON output strictly follows the provided schema for each tool.\n    - Each tool's schema will define the structure and required fields for the JSON output.\n\n    3. **Schema Example**:\n    If a tool named `example_tool` with a schema requires `param1` and `param2`, your response should look like:\n    ```json\n    {\n        \"type\": \"function\",\n        \"function\": {\n        \"name\": \"example_tool\",\n        \"parameters\": {\n            \"param1\": 123,\n            \"param2\": \"example_value\"\n        }\n        }\n    }\n    ```\n\n    4. **Error Handling**:\n    - If there is an error or the information provided by the user is insufficient to generate a valid JSON, respond with an appropriate error message in JSON format, also encapsulated in markdown.\n\n    Remember, clarity and adherence to the schema are paramount. Your primary goal is to ensure the user receives well-structured JSON outputs that align with the tool's requirements.\n\n    ---\n\n    Here is the format you should always follow for your responses involving tool usage:\n\n    ```json\n    {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"<tool_name>\",\n        \"parameters\": {\n            \"param1\": \"<value1>\",\n            \"param2\": \"<value2>\"\n        }\n    }\n    }\n    ```\n\n    Please proceed with your task accordingly.\n\n    ",
    "mode": "standard",
    "stopping_func": null,
    "tokenizer": null,
    "device_id": 0,
    "retry_attempts": 1,
    "reasoning_effort": null,
    "metadata_output_type": "json",
    "name": "ToT-Agent-20ff5ca784c045248fef48ea5f72495b",
    "auto_generate_prompt": false,
    "self_healing_enabled": false,
    "dynamic_context_window": true,
    "logs_to_filename": null,
    "dynamic_temperature_enabled": true,
    "algorithm_of_thoughts": false,
    "code_interpreter": false,
    "metadata": null,
    "time_created": "2025-10-28 17:40:49",
    "agent_output": null,
    "agent_name": "ToT-Agent-20ff5ca784c045248fef48ea5f72495b",
    "print_every_step": false,
    "summarize_multiple_images": false,
    "planning": false
}